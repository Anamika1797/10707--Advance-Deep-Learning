{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dbn import DBN, fit_mnist_dbn\n",
    "from neural_net import WarmUpMLPClassifier\n",
    "from rbm import RBM, binary_data, shuffle_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, path = None, cols=3, cmap='gray'):\n",
    "    rows = (len(images) + cols - 1) // cols\n",
    "    fig, ax = plt.subplots(rows, cols)\n",
    "    for i, image in enumerate(images):\n",
    "        ax[i//cols][i%cols].imshow(image, cmap=cmap)\n",
    "        ax[i//cols][i%cols].get_xaxis().set_ticks([])\n",
    "        ax[i//cols][i%cols].get_yaxis().set_ticks([])\n",
    "    for i in range(len(images), rows*cols):\n",
    "        ax[i//cols][i%cols].get_xaxis().set_ticks([])\n",
    "        ax[i//cols][i%cols].get_yaxis().set_ticks([])\n",
    "        ax[i//cols][i%cols].axis('off')\n",
    "    fig.set_size_inches(cols*10, rows*10)\n",
    "    if path is not None:\n",
    "        plt.savefig(path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x28cd3c63df0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.seterr(all='raise')\n",
    "train_data = np.genfromtxt('./data/digitstrain.txt', delimiter=\",\")\n",
    "train_X = train_data[:, :-1] \n",
    "train_Y = train_data[:, -1]\n",
    "train_X = binary_data(train_X)\n",
    "\n",
    "valid_data = np.genfromtxt('./data/digitsvalid.txt', delimiter=\",\")\n",
    "valid_X = valid_data[:, :-1]\n",
    "valid_X = binary_data(valid_X)\n",
    "valid_Y = valid_data[:, -1]\n",
    "\n",
    "test_data = np.genfromtxt('./data/digitstest.txt', delimiter=\",\")\n",
    "test_X = test_data[:, :-1]\n",
    "test_X = binary_data(test_X)\n",
    "test_Y = test_data[:, -1]\n",
    "\n",
    "plt.imshow(train_X[100].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 2., 3.]), array([300, 300, 300], dtype=int64))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_Y[300:1200],return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(0,1000):\n",
    "    plt.imshow(train_X[i].reshape(28, 28), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Local Autograder tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradescope-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run tests/run_tests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) RBM Task\n",
    "\n",
    "Two files, `rbm.py` and `dbn.py`, are provided. Only `rbm.py` will be evaluated in autograder, but you need to implement and submit both. You should read the instructions on top of these files, and the docstrings very carefully. You can change anything as you see fit in \\texttt{dbn.py}, as this file will not be autograded.\n",
    "\n",
    "We recommend you to complete and use the <span style=\"color:DarkOrange\"> RBM.fit </span>, <span style=\"color:DarkOrange\"> WarmUpMLPClassifier.fit </span> and <span style=\"color:DarkOrange\"> DBN.fit </span> methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Training RBM\n",
    "\n",
    "Try the RBM model with gibbs steps $k$ as 1, 3, and 5. For each $k$, plot reconstruction error against the epoch number for training and validation on one plot. So you should include 3 plots here, each contains two curves for training and validation. How does $k$ affect training convergence of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 100\n",
    "ks = [1, 3, 5]\n",
    "lr = 0.01\n",
    "max_epochs=40\n",
    "\n",
    "result = {}\n",
    "rbms = {}\n",
    "for i in ks:\n",
    "    rbm=RBM(n_visible=784, n_hidden=n_hidden, \n",
    "              k=i, lr=lr, max_epochs=max_epochs)\n",
    "    result[i]=rbm.fit(X=train_X, valid_X=valid_X)\n",
    "\n",
    "# Finish the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ks:\n",
    "    plt.plot(range(40),result[i][0],label='Train')\n",
    "    plt.plot(range(40),result[i][1],label='Test')\n",
    "    plt.title('Reconstruction Error vs Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Reconstruction Error')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Visualizing and understanding learned parameters\n",
    "\n",
    "Choose one model that you like, and visualize its learned $W$ as 100 images that are 28-by-28 in pixel. Plot all of them in one figure. What are being plotted here? Do they exhibit any structure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm_b=RBM(n_visible=784, n_hidden=n_hidden, \n",
    "              k=1, lr=lr, max_epochs=max_epochs)\n",
    "train_re,val_re=rbm_b.fit(X=train_X, valid_X=valid_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,100):\n",
    "    plt.imshow(rbm_b.W[i].reshape(28, 28), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Generation\n",
    "\n",
    "Set $k>1000$ for this task. Display the 100 generated samples for digit images in one figure. Do they look like handwritten digits? What if you retrain your RBM on only 3 digits, say $\\textbf{1, 2}$ and $\\textbf{3}$? If you train with $k=1$ vs $k=5$, do you see a difference in generated figures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm_c=RBM(n_visible=784, n_hidden=n_hidden, \n",
    "              k=1050, lr=lr, max_epochs=20)\n",
    "train_re,val_re=rbm_c.fit(X=train_X, valid_X=valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_1={}\n",
    "for i in range(len(train_X)):\n",
    "    h_prob_x=rbm_c.h_v(train_X[i])\n",
    "    h_sample_x=rbm_c.sample_h(h_prob_x)\n",
    "    v_prob_x=  rbm_c.v_h(h_sample_x)\n",
    "    v_sample_x = rbm_c.sample_v(v_prob_x)\n",
    "    sample_data_1[i]=v_sample_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm_c=RBM(n_visible=784, n_hidden=n_hidden, \n",
    "              k=1050, lr=lr, max_epochs=20)\n",
    "train_re,val_re=rbm_c.fit(X=train_X, valid_X=valid_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Conditional Generation\n",
    "\n",
    "Only reveal the top half of MNIST images (data generation code is provided to you), and use the RBM to reconstruct the bottom half of the image. Note here when you do gibbs sampling, when you sample $\\bf v$ condition on $\\bf h$, part of $\\bf v$ is known for sure. You need to inject these known value to the newly sampled $\\bf v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ims = []\n",
    "num_test = 10\n",
    "mask = np.zeros((28, 28))\n",
    "mask[0:14] = 1\n",
    "mask_1d = mask.reshape(-1)\n",
    "masked_X=[]\n",
    "for i in range(num_test):\n",
    "    masked_X.append(train_X[i*300])\n",
    "\n",
    "plot_images([(x*mask_1d).reshape(28,28) for x in masked_X], path = None, cols=5, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditioned on the observed pixels, complete the rest of the image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Supervised learning with RBM\n",
    "\n",
    "Take the RBM you have trained and initialize a 2-layer neural network, of which the first layer's weights are initialized using the RBM's weight. Compare the training trajectory of this RBM-initialized network with a randomly initialized network. Does the RBM-initialized network converge faster? Plot the training loss of these two networks in one figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rbm = RBM(n_visible=784, n_hidden=300,  k=5, lr=0.01, max_epochs=2)\n",
    "# rbm.fit(X=train_X, valid_X=valid_X)\n",
    "# rbm_clf = WarmUpMLPClassifier(lr=0.01, max_epochs=2, \n",
    "#                               hidden_layer_sizes=(300,),\n",
    "#                               Ws=[rbm.W,], hbiases=[rbm.hbias,])\n",
    "# rbm_clf.fit(train_X, train_Y)\n",
    "\n",
    "# plt.plot(rbm_clf.loss_curve_)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) DBN Task\n",
    "\n",
    "Truncate our dataset and only retain images of digits $\\textbf{7}, \\textbf{8}$, and $\\textbf{9}$.\n",
    "Build a DBN with two hidden layers with 500 and 784 units respectively, so there are two RBMs with 500 and 784 hidden units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Training DBN\n",
    "Training this DBN with gibbs steps $k=3$. For each RBM, plot reconstruction error against the epoch number for training and validation on one plot. So you should include 2 plots here, each contains two curves for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbn = DBN(n_visible=784, layers=layers, \n",
    "          k=k, max_epochs=max_epochs, lr=lr)\n",
    "dbn.fit(X=train_X, valid_X=valid_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Generation\n",
    "\n",
    "Set $k>1000$ for this task. Display the 100 generated samples for digit images in one figure. Do they look like handwritten digits? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
